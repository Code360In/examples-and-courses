{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Description\n\nНеобхідно побудувати модель, яка буде прогнозувати сегмент churn-водіїв, тобто водіїв, які перестануть користуватися сервісом.\n\nДля цього вам необхідно навчити модель, використовуючи дані з train.csv. Після цього, скориставшись вашим препроцессінгом та використовуючи вашу модель, потрібно передбачити для кожного Id з test.csv, що водій відноситься до сегменту churn-водіїв (1 - відноситься, 0 - не відноситься).\nЗверніть увагу, що необхідно спрогнозувати факт відношення до сегменту churn, без прив'язки до періоду (тижня).","metadata":{}},{"cell_type":"markdown","source":"## Evaluation\n\nThe evaluation metric for this competition is AUC (https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n\nФайл з результатами повинен містити:\n+ Id – ідентифікатор водія;\n+ Predicted - клас 1 або клас 0, яку визначає ваша модель.","metadata":{}},{"cell_type":"markdown","source":"## Data\n\ntrain.csv - the training set<br>\ntest.csv - the test set<br>\nSample_Submission.csv - a sample submission file in the correct format<br>\n\nФайл train.csv містить числові дані щодо роботи водіїв сервісу Уклон за 4 тижні.\n- Id – ідентифікатор водія;\n- Week – номер тижня (тижні послідовні, 0 – найновіший);\n- V1 - V22 та P1 - P27 – числові дані щодо роботи водіїв у відповідний період.\n- Target – значення цільової мітки (1 – churn , 0 – не churn).\n\nФайл test.csv містить дані:\n- Id – ідентифікатор водія;\n- Week – номер тижня (тижні послідовні, 0 – найновіший);\n- V1 - V22 та P1 - P27 – дані щодо роботи водіїв у відповідний період.\n\nФайл з результатами повинен містити:\n- Id – ідентифікатор водія;\n- Predicted - клас 1 або клас 0, яку визначає ваша модель.","metadata":{}},{"cell_type":"markdown","source":"## Module importing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport phik\nimport shap\nimport pickle\n\nimport lightgbm as lgb\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion, Pipeline, _fit_transform_one, _transform_one\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, classification_report, ConfusionMatrixDisplay\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport multiprocessing as mp\nfrom joblib import Parallel, delayed","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:12.935933Z","iopub.execute_input":"2022-01-23T19:10:12.936285Z","iopub.status.idle":"2022-01-23T19:10:16.561796Z","shell.execute_reply.started":"2022-01-23T19:10:12.936185Z","shell.execute_reply":"2022-01-23T19:10:16.560715Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:16.564110Z","iopub.execute_input":"2022-01-23T19:10:16.564342Z","iopub.status.idle":"2022-01-23T19:10:17.084269Z","shell.execute_reply.started":"2022-01-23T19:10:16.564312Z","shell.execute_reply":"2022-01-23T19:10:17.083185Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom tqdm import tqdm\nfrom functools import partial\nfrom typing import List, Dict, Tuple, Union, Optional","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:17.085622Z","iopub.execute_input":"2022-01-23T19:10:17.085906Z","iopub.status.idle":"2022-01-23T19:10:17.091588Z","shell.execute_reply.started":"2022-01-23T19:10:17.085872Z","shell.execute_reply":"2022-01-23T19:10:17.090522Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:17.093543Z","iopub.execute_input":"2022-01-23T19:10:17.093835Z","iopub.status.idle":"2022-01-23T19:10:17.103361Z","shell.execute_reply.started":"2022-01-23T19:10:17.093790Z","shell.execute_reply":"2022-01-23T19:10:17.102590Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Settings","metadata":{}},{"cell_type":"code","source":"PATH2DATA = Path('/kaggle/input/techuklon-int20h')\nPATH2OUTPUT = Path('/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:17.104322Z","iopub.execute_input":"2022-01-23T19:10:17.104570Z","iopub.status.idle":"2022-01-23T19:10:17.114929Z","shell.execute_reply.started":"2022-01-23T19:10:17.104538Z","shell.execute_reply":"2022-01-23T19:10:17.114049Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ID = 'Id'\nWEEK = 'Week'\nTARGET = 'target'\n\nservice_columns = [ID, WEEK, TARGET]\n\nSEED = 42\nMETRIC = 'auc'","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:18.437344Z","iopub.execute_input":"2022-01-23T19:10:18.437637Z","iopub.status.idle":"2022-01-23T19:10:18.443015Z","shell.execute_reply.started":"2022-01-23T19:10:18.437606Z","shell.execute_reply":"2022-01-23T19:10:18.442001Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(PATH2DATA / 'train.csv')\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:20.042663Z","iopub.execute_input":"2022-01-23T19:10:20.042985Z","iopub.status.idle":"2022-01-23T19:10:20.487038Z","shell.execute_reply.started":"2022-01-23T19:10:20.042953Z","shell.execute_reply":"2022-01-23T19:10:20.486192Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(PATH2DATA / 'test.csv')\ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:20.488598Z","iopub.execute_input":"2022-01-23T19:10:20.488981Z","iopub.status.idle":"2022-01-23T19:10:20.677835Z","shell.execute_reply.started":"2022-01-23T19:10:20.488950Z","shell.execute_reply":"2022-01-23T19:10:20.676708Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"numerical_columns = df_train.drop(columns=service_columns).select_dtypes(include=['float', 'int']).columns.tolist()\nassert len(numerical_columns)+3 == df_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:20.834550Z","iopub.execute_input":"2022-01-23T19:10:20.834873Z","iopub.status.idle":"2022-01-23T19:10:20.864681Z","shell.execute_reply.started":"2022-01-23T19:10:20.834839Z","shell.execute_reply":"2022-01-23T19:10:20.863794Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Simple EDA & Hypothesis\n\n- Гипотеза о том что могут значить фичи **P** и **V** (*p - фичи водителя, v - фичи транспортного средства ???*)\n- Добавить индикатор отсутствия фич P (*или можно прям количество фич писать, что по 6 фичам из P у нас пусто*)\n- Добавить индикатор отсутствия фич V\n- Поискать в данных колонки которые могут нести категориальную информацию (*часы, дни, бинарные колонки*)\n- Проверить распределение фич, может стоит к ним применить какое-то преобразование (`*np.log`, `np.exp` и тд*)\n- Подумать каких водителей стоит выбросить с обучающей выборки (*мало фич, все нули*)\n- Проверить наличие выбросов или подозрительных значений в колонках фич\n- Проверить матчится ли обучающая выборка на тестовую (*может часть водителей в обучающей так же присутствует в тестовой и это нам чет даст*)\n- Проверить у каждого ли водителя ровно 4 недели (*отдельно проверить тест и трейн*) - есть записей меньше 4, то делаем предположение что водитель только присоединился к уклону, скорее всего такие ребята не склонны к оттоку (*но это можно и перепроверить, посчитав долю таргета для таких ребят*)\n- Лаговые фичи: берем прошлые недели и считаем разницу по фичам (*гипотеза такая: если у человека есть постепенное проседание или постепенное увеличение показателей, то он склонен к оттоку*)\n- Идеи для агрегации по неделям (*поскольку нам нужно получить предсказания для каждого **Id**, а не пары **Id-Week***):\n    - просто посчитать среднее - это в целом проще всего\n    - считать `avg`, `min`, `max`, `std`, `sum` - это раздует количество фич в 4 раза, но похер вообще (~~живем один раз~~)\n    - просто перенести фичи с колонок в столбики тип: `p1_week1`, `p1_week2`, ...., `p54_week1`, `p54_week2`, ...\n- Мб будут идеи какие фичи можно добавить, кроме как попарные умножения\n- Возможно есть фичи которые не меняются в рамках этих 4 недель - тип city_id, который для таксиста будет один и тот же или car_id - айдишка его машины (*хз что это нам дает, но может будет осторожней юзать агрегации над ними*)\n- Подумать над стратегией работы с дисбалансом (учитывая что метрика ROC-AUC, то в целом можно сильно и не напрягаться, но у меня пока 2 основные идеи на уме):\n    - Делаем undersampling мажорного класса до минорного N раз (при этом рандомно семплируя данные с мажорного класса). На выходе получим N выборок, на которых строим N моделей, а финальные скоры усредняем. Получим такой тип ансамбль\n    - Поиграться в весами классов в моделях, чтобы добавить штрафы\n- Подумать над тем как будем валидировать результат. Я сильно не вникал в данные, но если не будет пересечения тестовой выборки с трейном, то я думаю что StratifiedKFold наше все\n- Подумать стоит ли делать проверку на гомогенность (когда распределение фич на трейне не совпадает с тестовыми данными). Просто они могли нарезать выборку для теста совсем с другого периода, где некоторые фичи будут ну прям совсем отличатся.","metadata":{}},{"cell_type":"markdown","source":"### Target","metadata":{}},{"cell_type":"code","source":"assert all(df_train.groupby(ID)[TARGET].count() == 4)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:23.966429Z","iopub.execute_input":"2022-01-23T19:10:23.966769Z","iopub.status.idle":"2022-01-23T19:10:23.977504Z","shell.execute_reply.started":"2022-01-23T19:10:23.966719Z","shell.execute_reply":"2022-01-23T19:10:23.976777Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=df_train[TARGET]);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:24.149902Z","iopub.execute_input":"2022-01-23T19:10:24.150310Z","iopub.status.idle":"2022-01-23T19:10:24.377682Z","shell.execute_reply.started":"2022-01-23T19:10:24.150273Z","shell.execute_reply":"2022-01-23T19:10:24.376761Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train[TARGET].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:24.379245Z","iopub.execute_input":"2022-01-23T19:10:24.379557Z","iopub.status.idle":"2022-01-23T19:10:24.390316Z","shell.execute_reply.started":"2022-01-23T19:10:24.379523Z","shell.execute_reply":"2022-01-23T19:10:24.389339Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Train data","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:24.596969Z","iopub.execute_input":"2022-01-23T19:10:24.597451Z","iopub.status.idle":"2022-01-23T19:10:24.633303Z","shell.execute_reply.started":"2022-01-23T19:10:24.597387Z","shell.execute_reply":"2022-01-23T19:10:24.632381Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train[numerical_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:24.759091Z","iopub.execute_input":"2022-01-23T19:10:24.759390Z","iopub.status.idle":"2022-01-23T19:10:24.953711Z","shell.execute_reply.started":"2022-01-23T19:10:24.759358Z","shell.execute_reply":"2022-01-23T19:10:24.952753Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def simple_na_report(df: pd.DataFrame) -> pd.core.series.Series:\n    t = df_train.isnull().sum() / len(df_train)\n    return t[t > 0]\n\nsimple_na_report(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:24.955276Z","iopub.execute_input":"2022-01-23T19:10:24.955540Z","iopub.status.idle":"2022-01-23T19:10:24.971441Z","shell.execute_reply.started":"2022-01-23T19:10:24.955510Z","shell.execute_reply":"2022-01-23T19:10:24.970451Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_train.isnull(), cbar=False);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:25.304230Z","iopub.execute_input":"2022-01-23T19:10:25.305072Z","iopub.status.idle":"2022-01-23T19:10:27.769771Z","shell.execute_reply.started":"2022-01-23T19:10:25.305009Z","shell.execute_reply":"2022-01-23T19:10:27.768878Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# for col in categorical_columns:\n#     if len(df_train[col].unique()) > 25:\n#         continue\n#     temp = df_train.groupby([col, TARGET]).count()[ID].reset_index()\n#     fig = px.bar(temp, x=col, y=ID, color=TARGET, title=f\"{col} - count of id`s\",width=1000, height=500)\n#     fig.show()\n# del temp","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:27.771609Z","iopub.execute_input":"2022-01-23T19:10:27.771880Z","iopub.status.idle":"2022-01-23T19:10:27.775891Z","shell.execute_reply.started":"2022-01-23T19:10:27.771838Z","shell.execute_reply":"2022-01-23T19:10:27.774879Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# print('Columns with high cardinality:')\n# high_cardinality_columns = []\n# for col in categorical_columns:\n#     unique_cnt = len(df_train[col].unique())\n#     if unique_cnt > 25:\n#         high_cardinality_columns.append(col)\n#         print(f'- {col}: {unique_cnt} unique values')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:27.777456Z","iopub.execute_input":"2022-01-23T19:10:27.777895Z","iopub.status.idle":"2022-01-23T19:10:27.793240Z","shell.execute_reply.started":"2022-01-23T19:10:27.777846Z","shell.execute_reply":"2022-01-23T19:10:27.792301Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# print('Columns which contain minor categories:')\n# minor_cat_columns = []\n# for col in categorical_columns:\n#     minor_cnt = len([*filter(lambda x: x < 5e-2, df_train[col].value_counts(normalize=True).values)])\n#     if minor_cnt > 0:\n#         minor_cat_columns.append(col)\n#         print(f'- {col}: {minor_cnt} minor categories')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:27.795599Z","iopub.execute_input":"2022-01-23T19:10:27.796537Z","iopub.status.idle":"2022-01-23T19:10:27.805491Z","shell.execute_reply.started":"2022-01-23T19:10:27.796489Z","shell.execute_reply":"2022-01-23T19:10:27.804539Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Test data","metadata":{}},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:32.775436Z","iopub.execute_input":"2022-01-23T19:10:32.776122Z","iopub.status.idle":"2022-01-23T19:10:32.808771Z","shell.execute_reply.started":"2022-01-23T19:10:32.776084Z","shell.execute_reply":"2022-01-23T19:10:32.807634Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_test[numerical_columns].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:32.945343Z","iopub.execute_input":"2022-01-23T19:10:32.945651Z","iopub.status.idle":"2022-01-23T19:10:33.107602Z","shell.execute_reply.started":"2022-01-23T19:10:32.945620Z","shell.execute_reply":"2022-01-23T19:10:33.106679Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def simple_na_report(df: pd.DataFrame) -> pd.core.series.Series:\n    t = df_test.isnull().sum() / len(df_test)\n    return t[t > 0]\nsimple_na_report(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:10:33.109029Z","iopub.execute_input":"2022-01-23T19:10:33.109281Z","iopub.status.idle":"2022-01-23T19:10:33.121895Z","shell.execute_reply.started":"2022-01-23T19:10:33.109248Z","shell.execute_reply":"2022-01-23T19:10:33.120913Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"class LogScaler:\n    def __init__(self, eps: float = 0.0):\n        self.eps = eps\n\n    def transform(self, x) -> pd.DataFrame:\n        return np.log10(x + self.eps)\n\n    def inverse_transform(self, x) -> pd.DataFrame:\n        return np.power(10, x) - self.eps","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:15:46.140715Z","iopub.execute_input":"2022-01-23T19:15:46.141265Z","iopub.status.idle":"2022-01-23T19:15:46.147252Z","shell.execute_reply.started":"2022-01-23T19:15:46.141232Z","shell.execute_reply":"2022-01-23T19:15:46.146421Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_names: List[str]):\n        self.feature_names = feature_names \n    \n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        return self\n    \n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> pd.DataFrame:\n        return X[self.feature_names]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:15:46.470714Z","iopub.execute_input":"2022-01-23T19:15:46.471037Z","iopub.status.idle":"2022-01-23T19:15:46.478217Z","shell.execute_reply.started":"2022-01-23T19:15:46.471004Z","shell.execute_reply":"2022-01-23T19:15:46.477084Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class FeatureDrop(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_names: List[str]):\n        self.feature_names = feature_names \n    \n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        return self\n    \n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> pd.DataFrame:\n        return X.drop(columns=self.feature_names)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:15:46.755561Z","iopub.execute_input":"2022-01-23T19:15:46.755970Z","iopub.status.idle":"2022-01-23T19:15:46.762968Z","shell.execute_reply.started":"2022-01-23T19:15:46.755933Z","shell.execute_reply":"2022-01-23T19:15:46.761949Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class NumericalFeaturesGenerator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.log_scaler = LogScaler(eps=10)\n        \n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        return self\n    \n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> pd.DataFrame:\n        X_temp = X.copy()\n        \n        p=[col for col in numerical_columns if col.startswith('P')]\n        v=[col for col in numerical_columns if col.startswith('V')]\n\n        X_temp['P_isnull']=X_temp[[ID, WEEK]+p].isnull().sum(axis=1)\n        X_temp['V_isnull']=X_temp[[ID, WEEK]+v].isnull().sum(axis=1)\n        \n        numerical_columns2=numerical_columns+['P_isnull', 'V_isnull']\n        \n        X_temp.loc[:, numerical_columns] = self.log_scaler.transform(X_temp[numerical_columns].fillna(0.0))\n        \n        df1 = X_temp.groupby([ID])[[ID]].max().reset_index(drop=True)\n\n        for col in numerical_columns2:\n            df1 = df1.merge(X_temp.groupby([ID])[col].max().reset_index().rename(columns={col:col+'_max'}), on=ID)\n            df1 = df1.merge(X_temp.groupby([ID])[col].min().reset_index().rename(columns={col:col+'_min'}), on=ID)\n            df1 = df1.merge(X_temp.groupby([ID])[col].mean().reset_index().rename(columns={col:col+'_mean'}), on=ID)\n            df1 = df1.merge(X_temp.groupby([ID])[col].std().reset_index().rename(columns={col:col+'_std'}), on=ID)\n            df1 = df1.merge(X_temp.groupby([ID])[col].sum().reset_index().rename(columns={col:col+'_sum'}), on=ID)\n            df1 = df1.merge(X_temp.groupby([ID])[col].median().reset_index().rename(columns={col:col+'_median'}), on=ID)\n            \n        df2 = X_temp[X_temp[WEEK] == 0]\n        df3 = X_temp[X_temp[WEEK] == 3][numerical_columns2+[ID]]\n        \n        df2.columns = [col+'_week_0' if col in numerical_columns2 else col for col in df2.columns]\n        df3.columns = [col+'_week_3' if col in numerical_columns2 else col for col in df3.columns]\n        X_temp=X_temp.set_index(ID)\n        for i in range(1, 4):\n            t=(X_temp[X_temp[WEEK] == 0][numerical_columns2] - X_temp[X_temp[WEEK] == i][numerical_columns2])\n            t.columns=[col+f'_week_0_minus_{i}' for col in t.columns]\n            df2=df2.merge(t.reset_index(), on=ID)\n            \n        for i in range(1, 4):\n            t=(X_temp[X_temp[WEEK] == 0][numerical_columns2] / X_temp[X_temp[WEEK] == i][numerical_columns2])\n            t.columns=[col+f'_week_0_div_{i}' for col in t.columns]\n            df2=df2.merge(t.reset_index(), on=ID)\n            \n        for i in range(1, 4):\n            t=(X_temp[X_temp[WEEK] == 0][numerical_columns2] * X_temp[X_temp[WEEK] == i][numerical_columns2])\n            t.columns=[col+f'_week_0_mul_{i}' for col in t.columns]\n            df2=df2.merge(t.reset_index(), on=ID)\n\n        df1=df1.merge(df2, on=ID).merge(df3, on=ID).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n           \n        return df1","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:33:52.662299Z","iopub.execute_input":"2022-01-23T19:33:52.662619Z","iopub.status.idle":"2022-01-23T19:33:52.687171Z","shell.execute_reply.started":"2022-01-23T19:33:52.662586Z","shell.execute_reply":"2022-01-23T19:33:52.686206Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"class SimpleImputer(BaseEstimator, TransformerMixin):\n    \n    def __init__(\n        self,\n        strategy: str = 'mean',\n        fill_value: Optional[Union[str, int, float]] = None,\n        missing_values: Optional[Union[str]] = None,\n        columns=[],\n    ):\n        from sklearn.impute import SimpleImputer as SI\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.missing_values = missing_values\n        self.imputer = SI(strategy=self.strategy, fill_value=self.fill_value,)\n    \n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        self.imputer.fit(X)\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        idx = X.index\n        result = pd.DataFrame(self.imputer.transform(X), columns=X.columns)\n        result['id'] = idx\n        return result.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:33:53.768160Z","iopub.execute_input":"2022-01-23T19:33:53.768470Z","iopub.status.idle":"2022-01-23T19:33:53.777500Z","shell.execute_reply.started":"2022-01-23T19:33:53.768438Z","shell.execute_reply":"2022-01-23T19:33:53.776811Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"numerical_pipeline = Pipeline(steps=[\n    ('num_features_generator', NumericalFeaturesGenerator()),\n#     ('imputer', SimpleImputer(strategy='constant', missing_values=np.nan, fill_value=0)),\n])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:33:53.943414Z","iopub.execute_input":"2022-01-23T19:33:53.943886Z","iopub.status.idle":"2022-01-23T19:33:53.948395Z","shell.execute_reply.started":"2022-01-23T19:33:53.943847Z","shell.execute_reply":"2022-01-23T19:33:53.947457Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# X_train, X_val, y_train, y_val = train_test_split(\n#     df_train.drop(columns=[TARGET]), df_train[TARGET], test_size=0.2,\n#     random_state=SEED, shuffle=True, stratify=df_train[TARGET]\n# )","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:33:54.938919Z","iopub.execute_input":"2022-01-23T19:33:54.939435Z","iopub.status.idle":"2022-01-23T19:33:54.943034Z","shell.execute_reply.started":"2022-01-23T19:33:54.939390Z","shell.execute_reply":"2022-01-23T19:33:54.942073Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# test_index = df_train[ID][: int(len(df_train[ID]) * 0.2)]\n# train_index = df_train[ID][int(len(df_train[ID]) * 0.2):]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:33:55.200239Z","iopub.execute_input":"2022-01-23T19:33:55.200947Z","iopub.status.idle":"2022-01-23T19:33:55.205417Z","shell.execute_reply.started":"2022-01-23T19:33:55.200900Z","shell.execute_reply":"2022-01-23T19:33:55.204622Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"numerical_pipeline.fit(df_train);\nX_train = numerical_pipeline.transform(df_train).reset_index(drop=True)\ny_train = X_train[TARGET].values\nX_train.drop([WEEK, TARGET],axis=1, inplace=True)\nX_train = X_train.set_index(ID)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:45:54.518709Z","iopub.execute_input":"2022-01-23T19:45:54.519181Z","iopub.status.idle":"2022-01-23T19:46:05.286934Z","shell.execute_reply.started":"2022-01-23T19:45:54.519146Z","shell.execute_reply":"2022-01-23T19:46:05.285809Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.2,\n    random_state=SEED, shuffle=True, stratify=y_train,\n)\nX_train.shape, X_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:05.288509Z","iopub.execute_input":"2022-01-23T19:46:05.288789Z","iopub.status.idle":"2022-01-23T19:46:05.380981Z","shell.execute_reply.started":"2022-01-23T19:46:05.288755Z","shell.execute_reply":"2022-01-23T19:46:05.379964Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### 1. KNN Features (optional)\n+ it may contain data leak","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.neighbors import NearestNeighbors\nfrom multiprocessing import Pool\nfrom sklearn.preprocessing import StandardScaler as SS\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-01-23T10:57:53.661514Z","iopub.execute_input":"2022-01-23T10:57:53.662447Z","iopub.status.idle":"2022-01-23T10:57:53.667702Z","shell.execute_reply.started":"2022-01-23T10:57:53.662403Z","shell.execute_reply":"2022-01-23T10:57:53.667068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StandardScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.scaler = SS()\n    \n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        self.scaler.fit(X)\n        return self\n    \n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> pd.DataFrame:\n        idx = X.index\n        result = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n        result['id'] = idx\n        return result.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T10:57:56.723651Z","iopub.execute_input":"2022-01-23T10:57:56.724109Z","iopub.status.idle":"2022-01-23T10:57:56.732639Z","shell.execute_reply.started":"2022-01-23T10:57:56.724075Z","shell.execute_reply":"2022-01-23T10:57:56.731933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NearestNeighborsFeats(BaseEstimator, TransformerMixin):\n    '''\n        This class should implement KNN features extraction \n    '''\n    def __init__(self, k_list=[3, 8, 32], metric='cosine', n_jobs=-1, n_classes=None, n_neighbors=None, eps=1e-6):\n        self.n_jobs = n_jobs\n        self.k_list = k_list\n        self.metric = metric\n        \n        if n_neighbors is None:\n            self.n_neighbors = max(k_list) \n        else:\n            self.n_neighbors = n_neighbors\n            \n        self.eps = eps        \n        self.n_classes_ = n_classes\n    \n    def fit(self, X, y):\n        '''\n            Set's up the train set and self.NN object\n        '''\n        # Create a NearestNeighbors (NN) object. We will use it in `predict` function \n        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n                                      metric=self.metric, \n                                      n_jobs=1, \n                                      algorithm='brute' if self.metric=='cosine' else 'auto')\n        self.NN.fit(X)\n        \n        # Store labels \n        self.y_train = y\n        \n        # Save how many classes we have\n        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n        \n        \n    def transform(self, X, y=None):       \n        '''\n            Produces KNN features for every object of a dataset X\n        '''\n        test_feats = []\n        if self.n_jobs == 1:\n            for idx in tqdm(X.index, position=0, leave=True):\n                return_dict = self.get_features_for_one(X.loc[[idx]])\n                test_feats.append(return_dict)\n        else:\n            pool = Pool(processes=self.n_jobs) \n            for idx in X.index:\n                test_feats.append(pool.apply_async(self.get_features_for_one, (X.loc[[idx]],)))\n            test_feats = [res.get() for res in tqdm(test_feats, position=0, leave=True)]\n        return pd.DataFrame(test_feats)\n        \n        \n    def get_features_for_one(self, x):\n        '''\n            Computes KNN features for a single object `x`\n        '''\n\n        NN_output = self.NN.kneighbors(x)\n        \n        # Vector of size `n_neighbors`\n        # Stores indices of the neighbors\n        neighs = NN_output[1][0]\n        \n        # Vector of size `n_neighbors`\n        # Stores distances to corresponding neighbors\n        neighs_dist = NN_output[0][0] \n\n        # Vector of size `n_neighbors`\n        # Stores labels of corresponding neighbors\n        neighs_y = self.y_train.iloc[neighs] \n        \n        # We will accumulate the computed features here\n        # Eventually it will be a list of lists or np.arrays\n        # and we will use np.hstack to concatenate those\n        return_dict = {}\n        \n        \n        ''' \n            1. Fraction of objects of every class.\n               It is basically a KNNСlassifiers predictions.\n        '''\n        for k in self.k_list:\n            feats = np.bincount(neighs_y[:k],minlength=self.n_classes)\n            feats  = feats / feats.sum()\n            \n            assert len(feats) == self.n_classes\n            for c in range(self.n_classes):\n                return_dict[f'knn_f1_k_{k}_n_{c}_{self.metric}'] = feats[c]\n        \n        \n        '''\n            2. Minimum distance to objects of each class\n               Find the first instance of a class and take its distance as features.\n               \n               If there are no neighboring objects of some classes, \n               Then set distance to that class to be 999.\n        '''\n        feats = []\n        for c in range(self.n_classes):\n            feat = neighs_dist[neighs_y == c][0] if (neighs_y == c).sum() > 0 else 999\n            feats.append(feat)\n            return_dict[f'knn_f2_n_{c}_{self.metric}'] = feat\n        \n        assert len(feats) == self.n_classes\n        \n        '''\n            3. Minimum *normalized* distance to objects of each class\n               As 3. but we normalize (divide) the distances\n               by the distance to the closest neighbor.\n               \n               If there are no neighboring objects of some classes, \n               Then set distance to that class to be 999.\n        '''\n        feats = []\n        for c in range(self.n_classes):\n            feat = neighs_dist[neighs_y == c][0] if (neighs_y == c).sum() > 0 else 999\n            if feat!= 999:\n                feat = feat / (self.eps + neighs_dist[0])\n            feats.append(feat)\n            return_dict[f'knn_f3_n_{c}_{self.metric}'] = feat\n        \n        assert len(feats) == self.n_classes\n        \n        \n        '''\n            4. \n               4.1 Distance to Kth neighbor\n                   Think of this as of quantiles of a distribution\n               4.2 Distance to Kth neighbor normalized by \n                   distance to the first neighbor\n        '''\n        for k in self.k_list:\n            \n            feat_41 = neighs_dist[k-1]\n            feat_42 = neighs_dist[k-1] / (neighs_dist[0] + self.eps)\n            \n            return_dict[f'knn_f41_k_{k}_{self.metric}'] = feat_41\n            return_dict[f'knn_f42_k_{k}_{self.metric}'] = feat_42\n        \n        '''\n            5. Mean distance to neighbors of each class for each K from `k_list` \n               For each class select the neighbors of that class among K nearest neighbors \n               and compute the average distance to those objects\n\n               If there are no objects of a certain class among K neighbors, set mean distance to 999\n        '''\n        for k in self.k_list:\n            numerator = np.zeros(self.n_classes)\n            denominator = np.full(self.n_classes, self.eps)\n            t = neighs_y[:k].max() + 1\n            numerator[:t] = np.bincount(neighs_y[:k], weights=neighs_dist[:k])\n            denominator[:t] = self.eps + np.bincount(neighs_y[:k])\n            feats = np.where(numerator>0, numerator/denominator, 999)\n            \n            assert len(feats) == self.n_classes\n            for c in range(self.n_classes):\n                return_dict[f'knn_f5_k_{k}_n_{c}_{self.metric}'] = feats[c]\n        \n        return return_dict","metadata":{"execution":{"iopub.status.busy":"2022-01-23T10:57:57.275897Z","iopub.execute_input":"2022-01-23T10:57:57.276474Z","iopub.status.idle":"2022-01-23T10:57:57.310124Z","shell.execute_reply.started":"2022-01-23T10:57:57.276423Z","shell.execute_reply":"2022-01-23T10:57:57.308959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_features_for_knn = best_50_features_baseline+filtered_features_1std\nlen(best_features_for_knn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knnf_cos = NearestNeighborsFeats(n_jobs=1, metric='cosine')\nknnf_min = NearestNeighborsFeats(n_jobs=1, metric='minkowski')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knnf_cos.fit(X_train, y_train)\nknnf_res_cos_test = knnf_cos.transform(X_test)\n\nknnf_res_cos_test[ID] = df_test.loc[:, ID].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knnf_min.fit(X_train, y_train)\nknnf_res_mink_test = knnf_min.transform(X_test)\n\nknnf_res_mink_test[ID] = df_test.loc[:, ID].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.merge(knnf_res_mink_test, on=ID, how='left')\ndf_test = df_test.merge(knnf_res_cos_test, on=ID, how='left')\ndf_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = KFold(n_splits=10, random_state=SEED, shuffle=True)\n\nknn_features_list_cos = []\nfor train_index, test_index in tqdm(skf.split(X=df_train, y=df_train[TARGET]), position=0, leave=True):\n    X_train_fold = df_train.loc[train_index, best_features_for_knn]\n    y_train_fold = df_train.loc[train_index, TARGET]\n\n    X_test_fold = df_train.loc[test_index, best_features_for_knn]\n    y_test_fold = df_train.loc[test_index, TARGET]\n    \n    print('Train/Val shapes:')\n    print((X_train_fold.shape, y_train_fold.shape), (X_test_fold.shape, y_test_fold.shape))\n    \n    print('Train/Val bad rates:')\n    print(y_train_fold.mean(), y_test_fold.mean())\n    \n    scaler = StandardScaler()\n    scaler.fit(X_train_fold)\n    X_train_fold = scaler.transform(X_train_fold)\n    X_test_fold = scaler.transform(X_test_fold)\n    \n    knnf = NearestNeighborsFeats(n_jobs=1, n_neighbors=8, metric='cosine')\n    knnf.fit(X_train_fold, y_train_fold)\n    knnf_fold_res = knnf.transform(X_test_fold)\n    \n    knnf_fold_res[ID] = df_train.loc[test_index, ID].values\n    \n    knn_features_list_cos.append(knnf_fold_res.copy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = KFold(n_splits=10, random_state=SEED, shuffle=True)\n\nknn_features_list_mink = []\nfor train_index, test_index in tqdm(skf.split(X=df_train, y=df_train[TARGET]), position=0, leave=True):\n    X_train_fold = df_train.loc[train_index, best_features_for_knn]\n    y_train_fold = df_train.loc[train_index, TARGET]\n\n    X_test_fold = df_train.loc[test_index, best_features_for_knn]\n    y_test_fold = df_train.loc[test_index, TARGET]\n\n    print('Train/Val shapes:')\n    print((X_train_fold.shape, y_train_fold.shape), (X_test_fold.shape, y_test_fold.shape))\n    \n    print('Train/Val bad rates:')\n    print(y_train_fold.mean(), y_test_fold.mean())\n    \n    scaler = StandardScaler()\n    scaler.fit(X_train_fold)\n    X_train_fold = scaler.transform(X_train_fold)\n    X_test_fold = scaler.transform(X_test_fold)\n    \n    knnf = NearestNeighborsFeats(n_jobs=1, n_neighbors=8, metric='minkowski')\n    knnf.fit(X_train_fold, y_train_fold)\n    knnf_fold_res = knnf.transform(X_test_fold)\n    \n    knnf_fold_res[ID] = df_train.loc[test_index, ID].values\n    \n    knn_features_list_mink.append(knnf_fold_res.copy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_features_mink_df = pd.concat(knn_features_list_mink)\nknn_features_cos_df = pd.concat(knn_features_list_cos)\nknn_features_cos_df.shape, knn_features_mink_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert knn_features_cos_df[knn_features_cos_df.id.notna()].shape[0] == knn_features_mink_df[knn_features_mink_df.id.notna()].shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(knn_features_mink_df, on=ID, how='left')\ndf_train = df_train.merge(knn_features_cos_df, on=ID, how='left')\ndf_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_features_columns = knn_features_mink_df.columns.to_list() + knn_features_cos_df.columns.to_list()\nknn_features_columns = [col for col in knn_features_columns if col != ID]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_correlated_columns(corr_matrix: pd.DataFrame, threshold: float) -> List[str]:\n    col_corr = set()\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return list(col_corr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_pearson_corr = df_train[knn_features_columns].corr()\nknn_pearson_corr_columns = get_correlated_columns(knn_pearson_corr, threshold=0.85)\nlen(knn_pearson_corr_columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_features_columns = [col for col in knn_features_columns if col not in knn_pearson_corr_columns]\n# base_columns += knn_features_columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"base_columns = np.asarray(X_train.columns.tolist())\nlen(base_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:07.157787Z","iopub.execute_input":"2022-01-23T19:46:07.158086Z","iopub.status.idle":"2022-01-23T19:46:07.165265Z","shell.execute_reply.started":"2022-01-23T19:46:07.158054Z","shell.execute_reply":"2022-01-23T19:46:07.164556Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"### 1. Remove quazi-constant features","metadata":{}},{"cell_type":"code","source":"qconstant_filter = VarianceThreshold(threshold=0.00001)\nqconstant_filter.fit(X_train[base_columns])\nqconstant_columns = [col for col in base_columns if col not in base_columns[qconstant_filter.get_support()]]\nlen(qconstant_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:07.503300Z","iopub.execute_input":"2022-01-23T19:46:07.503595Z","iopub.status.idle":"2022-01-23T19:46:07.645798Z","shell.execute_reply.started":"2022-01-23T19:46:07.503562Z","shell.execute_reply":"2022-01-23T19:46:07.644903Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"best_features = [col for col in base_columns if col not in qconstant_columns]\nlen(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:07.677789Z","iopub.execute_input":"2022-01-23T19:46:07.678073Z","iopub.status.idle":"2022-01-23T19:46:07.687249Z","shell.execute_reply.started":"2022-01-23T19:46:07.678041Z","shell.execute_reply":"2022-01-23T19:46:07.686620Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":"### 2. Remove high correlated features (Pearson)","metadata":{}},{"cell_type":"code","source":"def get_correlated_columns(corr_matrix: pd.DataFrame, threshold: float) -> List[str]:\n    col_corr = set()\n    \n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return list(col_corr)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:07.997182Z","iopub.execute_input":"2022-01-23T19:46:07.997509Z","iopub.status.idle":"2022-01-23T19:46:08.004660Z","shell.execute_reply.started":"2022-01-23T19:46:07.997476Z","shell.execute_reply":"2022-01-23T19:46:08.003536Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"pearson_corr = X_train[best_features].corr()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:08.167628Z","iopub.execute_input":"2022-01-23T19:46:08.167959Z","iopub.status.idle":"2022-01-23T19:46:12.971017Z","shell.execute_reply.started":"2022-01-23T19:46:08.167920Z","shell.execute_reply":"2022-01-23T19:46:12.970033Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 1, figsize=(35, 15))\n\nsns.heatmap(pearson_corr, center=0, square=True, cbar_kws={\"shrink\": .5}, ax=axs)\naxs.set_title('Pearson correlation matrix');","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:12.972923Z","iopub.execute_input":"2022-01-23T19:46:12.973150Z","iopub.status.idle":"2022-01-23T19:46:18.692898Z","shell.execute_reply.started":"2022-01-23T19:46:12.973119Z","shell.execute_reply":"2022-01-23T19:46:18.691839Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"pearson_corr_columns = get_correlated_columns(pearson_corr, threshold=0.92)\nlen(pearson_corr_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:18.694141Z","iopub.execute_input":"2022-01-23T19:46:18.694379Z","iopub.status.idle":"2022-01-23T19:46:26.583244Z","shell.execute_reply.started":"2022-01-23T19:46:18.694347Z","shell.execute_reply":"2022-01-23T19:46:26.582159Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"best_features = [col for col in best_features if col not in pearson_corr_columns]\nlen(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:26.586280Z","iopub.execute_input":"2022-01-23T19:46:26.586720Z","iopub.status.idle":"2022-01-23T19:46:26.597352Z","shell.execute_reply.started":"2022-01-23T19:46:26.586669Z","shell.execute_reply":"2022-01-23T19:46:26.596280Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"### 3. Remove high correlated features (Phik)","metadata":{}},{"cell_type":"code","source":"phik_corr = X_train[best_features].phik_matrix()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:46:26.598466Z","iopub.execute_input":"2022-01-23T19:46:26.598701Z","iopub.status.idle":"2022-01-23T19:48:53.209763Z","shell.execute_reply.started":"2022-01-23T19:46:26.598670Z","shell.execute_reply":"2022-01-23T19:48:53.208535Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 1, figsize=(35, 15))\n\nsns.heatmap(phik_corr, center=0, square=True, cbar_kws={\"shrink\": .5}, ax=axs)\naxs.set_title('Phik correlation matrix');","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:48:53.211371Z","iopub.execute_input":"2022-01-23T19:48:53.212347Z","iopub.status.idle":"2022-01-23T19:48:57.438178Z","shell.execute_reply.started":"2022-01-23T19:48:53.212290Z","shell.execute_reply":"2022-01-23T19:48:57.437460Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"phik_corr_columns = get_correlated_columns(phik_corr, threshold=0.90)\nlen(phik_corr_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:48:57.439353Z","iopub.execute_input":"2022-01-23T19:48:57.439689Z","iopub.status.idle":"2022-01-23T19:48:58.570511Z","shell.execute_reply.started":"2022-01-23T19:48:57.439657Z","shell.execute_reply":"2022-01-23T19:48:58.569827Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"best_features = [col for col in best_features if col not in phik_corr_columns]\nlen(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:48:58.571925Z","iopub.execute_input":"2022-01-23T19:48:58.572150Z","iopub.status.idle":"2022-01-23T19:48:58.579125Z","shell.execute_reply.started":"2022-01-23T19:48:58.572119Z","shell.execute_reply":"2022-01-23T19:48:58.578010Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"### 4. Homogenity transformer","metadata":{}},{"cell_type":"code","source":"from abc import ABC, abstractmethod, ABCMeta\nfrom scipy import stats\nfrom statsmodels.stats.multitest import multipletests\nfrom sklearn.utils.multiclass import type_of_target\nfrom typing import Iterable, Optional, List, Tuple\nimport itertools\nimport operator\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\n\nclass Transformer(ABC):\n\n    @abstractmethod\n    def fit(self, ds):\n        pass\n\n    @abstractmethod\n    def transform(self, ds):\n        pass\n\n\nclass FeatureSelectionTransformer(Transformer):\n    def __init__(self):\n        self.features_in = []\n        self.features_out = []\n\n    def fit(self, ds, ds_test=None):\n        self.features_in = ds.columns.tolist()\n        self.features_out = self.features_in.copy()\n        return self\n\n    def transform(self, ds):\n        if len(self.features_out) > 0:\n            return ds[self.features_out]\n        else:\n            return ds\n\n    def __repr__(self):\n        return \"{0}\".format(self.__class__.__name__) + (\", Features={0}->{1}\".format(len(self.features_in), len(self.features_out)) if len(self.features_in) > 0 and len(self.features_out) > 0 else \"\")\n\n\nclass HomogeneityThresholdTransformer(FeatureSelectionTransformer):\n\n    def __init__(\n            self,\n            hypothesis_threshold: float = 0.85,\n            p_val_threshold: float = 0.01,\n            sample_size: int = 2000,\n            n_repeat: int = 20,\n            voting_threshold: float = 0.35,\n            use_multiple_tests: bool = True,\n            multiple_tests_method: str = 'bonferroni',\n            check_hypothesis_per_target: bool = False,\n            fold_threshold: float = 0.6,\n            test_functions_cont: Optional[Iterable] = None,\n            test_functions_cat: Optional[Iterable] = None,\n            n_splits: int = 5,\n            fold_method: ABCMeta = KFold,\n            random_seed: int = 42,\n    ):\n        super(HomogeneityThresholdTransformer, self).__init__()\n        self.hypothesis_threshold = hypothesis_threshold\n        self.p_val_threshold = p_val_threshold\n        self.sample_size = sample_size\n        self.n_repeat = n_repeat\n        self.voting_threshold = voting_threshold\n        self.use_multiple_tests = use_multiple_tests\n        self.multiple_tests_method = multiple_tests_method\n        self.check_hypothesis_per_target = check_hypothesis_per_target\n        self.fold_threshold = fold_threshold\n\n        self.test_functions_cont = {\n            HomogeneityThresholdTransformer.__ks_2samp_test,\n            HomogeneityThresholdTransformer.__anderson_ksamp_test,\n            HomogeneityThresholdTransformer.__mannwhitneyu_test,\n        } if test_functions_cont is None else test_functions_cont\n\n        self.test_functions_cat = {\n            HomogeneityThresholdTransformer.__chisquare_test\n        } if test_functions_cat is None else test_functions_cat\n        \n        self.n_splits = n_splits\n        self.fold_method = fold_method\n        self.random_seed = random_seed\n\n    @staticmethod\n    def __ks_2samp_test(samples):\n        \"\"\"\n        This is a two-sided test for the null hypothesis that 2 independent samples\n        are drawn from the same continuous distribution.\n        \"\"\"\n        sample1, sample2 = samples[0], samples[1]\n        statistic, p_val = stats.ks_2samp(sample1, sample2, mode='asymp', alternative='two-sided')\n        return statistic, p_val\n\n    @staticmethod\n    def __mannwhitneyu_test(samples):\n        \"\"\"\n        H0: The two populations are equal versus\n        H1: The two populations are not equal.\n        \"\"\"\n        sample1, sample2 = samples[0], samples[1]\n        statistic, p_val = stats.mannwhitneyu(sample1, sample2, alternative='two-sided')\n        return statistic, p_val\n\n    @staticmethod\n    def __anderson_ksamp_test(samples):\n        \"\"\"\n        The k-sample Anderson-Darling test is a modification of the\n        one-sample Anderson-Darling test. It tests the null hypothesis\n        that k-samples are drawn from the same population without having\n        to specify the distribution function of that population.\n        \"\"\"\n        statistic, _, p_val = stats.anderson_ksamp(samples=samples)\n        return statistic, p_val\n\n    @staticmethod\n    def __chisquare_test(samples):\n        \"\"\"\n        Chi-square test of independence of variables in a contingency table.\n        \"\"\"\n\n        sample1, sample2 = samples[0], samples[1]\n\n        value_counts1 = dict(zip(*np.unique(sample1, return_counts=True)))\n        value_counts2 = dict(zip(*np.unique(sample2, return_counts=True)))\n\n        f1, f2 = [], []\n        for category_name in value_counts1.keys():\n            f1.append(value_counts1.get(category_name))\n            f2.append(value_counts2.get(category_name, 0))\n\n        statistic, p_val, _, _ = stats.chi2_contingency([f1, f2])\n        return statistic, p_val\n\n    def __is_pass_stat_test(self, test_fn, *args) -> Tuple[bool, bool]:\n        if len(args) < 2:\n            raise ValueError(f'Tuple `args` must contain at least 2 elements, but it contains {len(args)}')\n\n        # prepare each sample\n        samples = [feature_sample.dropna() for feature_sample in args]\n\n        # if some sample has zero-length --> skip test computation\n        skip_sampling = not all(list(map(bool, map(len, samples))))\n        if skip_sampling:\n            # is_pass = False, skip_flag = True\n            return False, True\n\n        p_vals = []\n        for repeat_id in range(self.n_repeat):\n            # get subsample for each sample\n            prepared_samples = [feature_sample.sample(self.sample_size, replace=True, random_state=self.random_seed).values\n                                for feature_sample in samples]\n\n            # check that all numbers aren't identical\n            identical_numbers_ind = any([len(np.unique(feature_sample)) < 2 for feature_sample in prepared_samples])\n\n            if not identical_numbers_ind:\n                # compute test\n                _, p_val = test_fn(prepared_samples)\n                p_vals.append(p_val)\n\n        # if some sample has identical numbers --> skip test computation\n        if len(p_vals) == 0:\n            # is_pass = False, skip_flag = True\n            return False, True\n\n        if self.use_multiple_tests:\n            # true for hypothesis that can be rejected for given p_val_threshold\n            reject, _, _, _ = multipletests(p_vals, alpha=self.p_val_threshold, method=self.multiple_tests_method, )\n            result = np.mean(reject) < self.hypothesis_threshold\n        else:\n            result = ((np.array(p_vals) <= self.p_val_threshold).sum() / len(p_vals)) < self.hypothesis_threshold\n\n        # is_pass = True/False, skip_flag = False\n        return result, False\n\n    def __feature_test(self, train, test, features_cont, features_cat) -> List[Tuple[str, bool]]:\n        def __test(features, test_functions):\n            features_out = []\n            for feature in tqdm(features, position=0, leave=True, desc='features progress'):\n                passed_tests_cnt = 0.0\n                not_skipped_test_cnt = 0.0\n                for test_fn in test_functions:\n                    # save two indicators:\n                    # 1. is_pass - train and test sets have same distribution for each test\n                    # 2. skip_flag - we want to skip this test\n                    is_pass, skip_flag = self.__is_pass_stat_test(\n                        test_fn,\n                        train[feature],\n                        test[feature],\n                    )\n                    if not skip_flag:\n                        passed_tests_cnt += is_pass\n                        not_skipped_test_cnt += 1\n\n                # save tuple (feature_name, skip_fold_flag)\n                if not_skipped_test_cnt == 0:\n                    features_out.append((feature, True))\n                elif (passed_tests_cnt / not_skipped_test_cnt) > self.voting_threshold:\n                    features_out.append((feature, False))\n            return features_out\n\n        features_out_cont = __test(features_cont, self.test_functions_cont)\n        features_out_cat = __test(features_cat, self.test_functions_cat)\n\n        return features_out_cont + features_out_cat\n\n    def __feature_test_per_target(self, train, test, features_cont, features_cat, target_col) -> List[Tuple[str, bool]]:\n        def __test(features, test_functions):\n            features_out = []\n            for feature in tqdm(features, position=0, leave=True, desc='features progress'):\n                passed_tests_result = dict.fromkeys(unique_target_values, [0.0, 0.0])\n                for target_val in passed_tests_result.keys():\n                    for test_fn in test_functions:\n                        # save two indicators:\n                        # 1. is_pass - train and test sets have same distribution for each test\n                        # 2. skip_flag - we want to skip this test\n                        is_pass, skip_flag = self.__is_pass_stat_test(\n                            test_fn,\n                            train.loc[train[target_col] == target_val, feature],\n                            test.loc[test[target_col] == target_val, feature],\n                        )\n                        if not skip_flag:\n                            passed_tests_result[target_val][0] += is_pass\n                            passed_tests_result[target_val][1] += 1\n\n                passed_tests_result = np.array(list(passed_tests_result.values()))\n                # # save tuple (feature_name, skip_fold_flag)\n                if any(passed_tests_result[:, 1] == 0):\n                    features_out.append((feature, True))\n                elif all((passed_tests_result[:, 0] / passed_tests_result[:, 1]) > self.voting_threshold):\n                    features_out.append((feature, False))\n            return features_out\n\n        unique_target_values = train[target_col].unique()\n        features_out_cont = __test(features_cont, self.test_functions_cont)\n        features_out_cat = __test(features_cat, self.test_functions_cat)\n\n        return features_out_cont + features_out_cat\n\n    @staticmethod\n    def __accumulate(l):\n        it = itertools.groupby(l, operator.itemgetter(0))\n        for key, subiter in it:\n            item_sum = 0\n            item_cnt = 0\n            for item in subiter:\n                item_sum += item[1]\n                item_cnt += 1\n            yield key, item_sum, item_cnt\n\n    def __check_hypothesis_by_fold(self, ds, features_cont, features_cat, target_col):\n        folds_features_out = []\n        kf = self.fold_method(n_splits=self.n_splits)\n        features_all = features_cont+features_cat\n        for train_index, test_index in tqdm(kf.split(ds), position=0, leave=True, desc='kfolds progress'):\n            if self.check_hypothesis_per_target:\n                folds_features_out.extend(\n                    self.__feature_test_per_target(ds.iloc[train_index], ds.iloc[test_index], features_cont, features_cat, target_col)\n                )\n            else:\n                folds_features_out.extend(\n                    self.__feature_test(ds.iloc[train_index], ds.iloc[test_index], features_cont, features_cat)\n                )\n\n        f_names, f_skipped, f_counts = np.array(list(\n            HomogeneityThresholdTransformer.__accumulate(sorted(folds_features_out))\n        )).T\n        f_skipped = f_skipped.astype(int)\n        f_counts = f_counts.astype(int)\n\n        f_counts = f_counts - f_skipped\n        f_not_skipped = self.n_splits - f_skipped\n        return f_names[((f_counts / f_not_skipped) > self.fold_threshold) | (f_not_skipped == 0)].tolist()\n\n    def __check_hypothesis_by_train(self, ds, ds_test, features_cont, features_cat, target_col):\n        train = ds\n        test = ds_test\n\n        if self.check_hypothesis_per_target:\n            test_features_out = self.__feature_test_per_target(train, test,\n                                                               features_cont, features_cat, target_col)\n        else:\n            test_features_out = self.__feature_test(train, test, features_cont, features_cat)\n\n        f_names, _, _ = np.array(list(\n            HomogeneityThresholdTransformer.__accumulate(test_features_out)\n        )).T\n\n        return f_names.tolist()\n\n    def fit(self, ds, ds_test=None):\n        super(HomogeneityThresholdTransformer, self).fit(ds=ds, ds_test=ds_test)\n\n        self.features_out = []\n        features_cont = ds.columns.tolist()\n        features_cat = []\n\n        target_col = [TARGET]\n        if len(target_col) > 1:\n            raise ValueError('Cannot deal with multidimensional target.')\n        else:\n            target_col = target_col[0]\n        features_cont = [col for col in features_cont if col != target_col]\n\n        if type_of_target(ds[target_col]) == 'continuous' and self.check_hypothesis_per_target:\n            raise ValueError('It is not possible to check hypothesis for continuous target.')\n\n        folds_features_out = self.__check_hypothesis_by_fold(ds, features_cont, features_cat, target_col)\n\n        if ds_test is not None:\n            test_features_out = self.__check_hypothesis_by_train(ds, ds_test, features_cont, features_cat, target_col)\n            self.features_out = list(set(test_features_out).intersection(folds_features_out))\n        else:\n            self.features_out = folds_features_out\n            \n        self.failed_features = [col for col in features_cont if col not in self.features_out]\n\n        return self\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:48:58.582910Z","iopub.execute_input":"2022-01-23T19:48:58.583306Z","iopub.status.idle":"2022-01-23T19:48:58.648431Z","shell.execute_reply.started":"2022-01-23T19:48:58.583254Z","shell.execute_reply":"2022-01-23T19:48:58.647386Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"htt = HomogeneityThresholdTransformer(check_hypothesis_per_target=True, random_seed=SEED)\nX_train_temp = X_train.copy().reset_index(drop=True)[best_features]\nX_train_temp[TARGET] = y_train\nX_val_temp = X_val.copy().reset_index(drop=True)[best_features]\nX_val_temp[TARGET] = y_val\nhtt.fit(ds=X_train_temp, ds_test=X_val_temp)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:51:33.118581Z","iopub.execute_input":"2022-01-23T19:51:33.118917Z","iopub.status.idle":"2022-01-23T19:58:52.527452Z","shell.execute_reply.started":"2022-01-23T19:51:33.118882Z","shell.execute_reply":"2022-01-23T19:58:52.526533Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"best_features = [col for col in htt.features_out if (col != TARGET) and (col in best_features)]\nlen(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.529950Z","iopub.execute_input":"2022-01-23T19:58:52.530200Z","iopub.status.idle":"2022-01-23T19:58:52.537792Z","shell.execute_reply.started":"2022-01-23T19:58:52.530162Z","shell.execute_reply":"2022-01-23T19:58:52.537118Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# htt = HomogeneityThresholdTransformer(check_hypothesis_per_target=False, random_seed=SEED)\n# X_train_temp = X_train.copy().reset_index(drop=True)\n# X_train_temp[TARGET] = y_train\n# htt.fit(ds=X_train_temp, ds_test=X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.538964Z","iopub.execute_input":"2022-01-23T19:58:52.539173Z","iopub.status.idle":"2022-01-23T19:58:52.551295Z","shell.execute_reply.started":"2022-01-23T19:58:52.539145Z","shell.execute_reply":"2022-01-23T19:58:52.550155Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"# base_columns = [col for col in htt.features_out if col != TARGET]\n# len(base_columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.554612Z","iopub.execute_input":"2022-01-23T19:58:52.554935Z","iopub.status.idle":"2022-01-23T19:58:52.562695Z","shell.execute_reply.started":"2022-01-23T19:58:52.554891Z","shell.execute_reply":"2022-01-23T19:58:52.561992Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"markdown","source":"### 5. Feature Selection - permutation + top-50","metadata":{}},{"cell_type":"code","source":"class PermutationImportanceFeatureSelectionTransformer():\n    \n    def __init__(self, model, n_repeats=10, scoring=roc_auc_score, random_state=42, n_jobs=1):\n        super(PermutationImportanceFeatureSelectionTransformer, self).__init__()\n        self.model = model\n        self.n_repeats = n_repeats\n        self.scoring = scoring\n        self.seed = random_state\n        self.n_jobs = n_jobs\n        self.importances = {}\n        self.features_in = []\n        self.features_out = []\n\n    def fit(self, X, y):\n        self.features_in = X.columns.tolist()\n        self.features_out = self.features_in.copy()\n\n        y_pred = self.model.predict(X)\n        baseline_score = self.scoring(y, y_pred)\n\n        self.features = self.model.feature_name()\n\n        importances = {}\n        importances_parallel = Parallel(n_jobs=self.n_jobs)(delayed(self.loop_iter)(X, y, baseline_score) for _ in tqdm(range(self.n_repeats), position=0, leave=True))\n            \n        for importance in tqdm(importances_parallel):\n            for feature in importance:\n                if feature not in importances.keys():\n                    importances[feature] = [importance[feature]]\n                else:\n                    importances[feature].append(importance[feature])\n\n        self.features_out = []\n        for k, v in tqdm(importances.items()):\n            if np.mean(v) - np.std(v) > 0:\n                self.features_out.append(k)\n\n        self.importances = {f: (np.mean(v), np.std(v)) for f, v in importances.items() if f in self.features_out}\n\n        return self\n    \n    def loop_iter(self, X, y, baseline_score):\n        importances = dict()\n        for feature in self.features:\n            X_perm = self.permute_feature(X=X, feature=feature)\n            y_perm = self.model.predict(X_perm)\n            \n            feature_score = self.scoring(y, y_perm)\n            importances[feature] = baseline_score - feature_score\n        return importances\n                \n    def permute_feature(self, X, feature):\n        X_perm = X.copy()\n        X_perm[feature] = np.random.permutation(X_perm[feature].values)\n        return X_perm\n\n    def __repr__(self):\n        return \"{0} by {1}\".format(self.__class__.__name__, self.model) + (\", Features={0}->{1}\".format(len(self.features_in), len(self.features_out)) if len(self.features_in) > 0 and len(self.features_out) > 0 else \"\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.564183Z","iopub.execute_input":"2022-01-23T19:58:52.564441Z","iopub.status.idle":"2022-01-23T19:58:52.584776Z","shell.execute_reply.started":"2022-01-23T19:58:52.564408Z","shell.execute_reply":"2022-01-23T19:58:52.583688Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"def get_best_perm_features(r, n_std=2, show_log=False) -> List[str]:\n    best_features = list()\n    importance = []\n    for k, v in r.items():\n        importance.append((k, v[0], v[1]))\n    importance = sorted(importance, key=lambda x: x[1], reverse=True)\n\n    for feature, mean, std in importance:\n        if mean - n_std * std > 0:\n            if show_log:\n                print(f\"{feature:<8}\", f\"{mean:.5f}\", f\" +/- {std:.5f}\")\n            best_features.append(feature)    \n    return best_features","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.587887Z","iopub.execute_input":"2022-01-23T19:58:52.588575Z","iopub.status.idle":"2022-01-23T19:58:52.601658Z","shell.execute_reply.started":"2022-01-23T19:58:52.588537Z","shell.execute_reply":"2022-01-23T19:58:52.600622Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"fit = lgb.Dataset(\n    data=X_train[best_features], label=y_train,\n)\n\nval = lgb.Dataset(\n    data=X_val[best_features], label=y_val,\n    reference=fit,\n)\n\nmodel = lgb.train(\n    params={'seed': SEED, 'objective': 'binary', 'is_unbalance': True, 'max_depth': -1, 'feature_fraction': 0.5,  'learning_rate': 0.01,  'zero_as_missing': False, 'boosting_type': 'gbdt', 'metric': [METRIC]},\n    train_set=fit,\n    num_boost_round=5000,\n    valid_sets=(fit, val),\n    valid_names=('train', 'val'),\n    early_stopping_rounds=25,\n    verbose_eval=25,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:58:52.603820Z","iopub.execute_input":"2022-01-23T19:58:52.604157Z","iopub.status.idle":"2022-01-23T19:59:01.027482Z","shell.execute_reply.started":"2022-01-23T19:58:52.604110Z","shell.execute_reply":"2022-01-23T19:59:01.026762Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"lgb.plot_importance(model, max_num_features=25);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:01.028722Z","iopub.execute_input":"2022-01-23T19:59:01.029141Z","iopub.status.idle":"2022-01-23T19:59:01.632487Z","shell.execute_reply.started":"2022-01-23T19:59:01.029108Z","shell.execute_reply":"2022-01-23T19:59:01.631114Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"best_10_features_baseline = list(map(lambda x: x[0], [(k, v) for k, v in sorted(zip(model.feature_name(), model.feature_importance()), key=lambda x: x[1], reverse=True) if v > 0][:10]))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:01.633819Z","iopub.execute_input":"2022-01-23T19:59:01.634102Z","iopub.status.idle":"2022-01-23T19:59:01.641465Z","shell.execute_reply.started":"2022-01-23T19:59:01.634069Z","shell.execute_reply":"2022-01-23T19:59:01.640531Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"r = PermutationImportanceFeatureSelectionTransformer(model, n_repeats=4, random_state=SEED, n_jobs=4, scoring=roc_auc_score)\nr.fit(X_val[best_features], y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:01.645244Z","iopub.execute_input":"2022-01-23T19:59:01.645562Z","iopub.status.idle":"2022-01-23T19:59:49.489106Z","shell.execute_reply.started":"2022-01-23T19:59:01.645529Z","shell.execute_reply":"2022-01-23T19:59:49.488027Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"filtered_features = get_best_perm_features(r.importances, show_log=True)\nlen(filtered_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:49.491038Z","iopub.execute_input":"2022-01-23T19:59:49.491703Z","iopub.status.idle":"2022-01-23T19:59:49.525524Z","shell.execute_reply.started":"2022-01-23T19:59:49.491647Z","shell.execute_reply":"2022-01-23T19:59:49.521840Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"best_features = list(set(filtered_features + best_10_features_baseline))\nlen(best_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:49.528082Z","iopub.execute_input":"2022-01-23T19:59:49.528449Z","iopub.status.idle":"2022-01-23T19:59:49.535804Z","shell.execute_reply.started":"2022-01-23T19:59:49.528399Z","shell.execute_reply":"2022-01-23T19:59:49.534759Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"pd.concat([X_train[best_features].reset_index(drop=True), pd.DataFrame(y_train, columns=[TARGET])], axis=1).corr().style.background_gradient(cmap='coolwarm').set_precision(3)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:49.537018Z","iopub.execute_input":"2022-01-23T19:59:49.537347Z","iopub.status.idle":"2022-01-23T19:59:50.076295Z","shell.execute_reply.started":"2022-01-23T19:59:49.537300Z","shell.execute_reply":"2022-01-23T19:59:50.075285Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"print('[', end='')\nfor feature in sorted(best_features):\n    print(f\"'{feature}'\", end=',\\n')\nprint(']')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:50.077815Z","iopub.execute_input":"2022-01-23T19:59:50.078573Z","iopub.status.idle":"2022-01-23T19:59:50.098934Z","shell.execute_reply.started":"2022-01-23T19:59:50.078534Z","shell.execute_reply":"2022-01-23T19:59:50.097894Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"best_features = ['P10_sum',\n'P10_week_0',\n'P10_week_0_mul_1',\n'P10_week_0_mul_2',\n'P10_week_3',\n'P11_min',\n'P11_week_0_minus_3',\n'P11_week_0_mul_2',\n'P11_week_0_mul_3',\n'P13_max',\n'P13_week_0_minus_1',\n'P13_week_0_minus_2',\n'P13_week_0_minus_3',\n'P15_week_0_minus_1',\n'P15_week_0_minus_2',\n'P16_max',\n'P16_min',\n'P16_week_0_minus_1',\n'P16_week_0_minus_2',\n'P16_week_3',\n'P17_mean',\n'P17_week_0_minus_1',\n'P17_week_0_mul_2',\n'P18_max',\n'P19_max',\n'P19_min',\n'P19_week_0_minus_1',\n'P19_week_0_minus_2',\n'P19_week_3',\n'P1_max',\n'P1_mean',\n'P1_min',\n'P1_week_0_minus_2',\n'P1_week_0_mul_1',\n'P21_max',\n'P21_std',\n'P21_week_0_minus_1',\n'P21_week_0_minus_2',\n'P22_max',\n'P22_week_0_minus_2',\n'P25_std',\n'P25_week_0',\n'P27_min',\n'P27_std',\n'P2_max',\n'P2_week_0_mul_3',\n'P3_std',\n'P4_max',\n'P5_week_0_minus_3',\n'P6_mean',\n'P6_std',\n'P7_median',\n'P8_max',\n'P8_week_0_minus_3',\n'P9_max',\n'V10_sum',\n'V11_max',\n'V14_sum',\n'V19_sum',\n'V21_max',\n'V3_sum',\n'V9_max',\n]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:50.100424Z","iopub.execute_input":"2022-01-23T19:59:50.100687Z","iopub.status.idle":"2022-01-23T19:59:50.110528Z","shell.execute_reply.started":"2022-01-23T19:59:50.100654Z","shell.execute_reply":"2022-01-23T19:59:50.109757Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters tuning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:50.112229Z","iopub.execute_input":"2022-01-23T19:59:50.112484Z","iopub.status.idle":"2022-01-23T19:59:50.635788Z","shell.execute_reply.started":"2022-01-23T19:59:50.112454Z","shell.execute_reply":"2022-01-23T19:59:50.634899Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"def get_best_params(X_train, y_train, best_features, n_splits, study):\n    def optuna_objective(trial):\n        params = {\n        'objective': 'binary',\n        'zero_as_missing': False,\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'is_unbalance': True,\n        'seed': SEED,\n        'metric': METRIC,\n        'max_depth': -1,  # {'default': -1, 'options': TunerNumberAxis(low=1, high=9, distribution='int')},\n        'learning_rate': trial.suggest_loguniform('learning_rate', low=1e-5, high=0.2),\n        'num_leaves': trial.suggest_categorical('num_leaves', [8, 16, 32, 64, 128, 256, 512]),#('num_leaves', [2, 4, 8, 16, 32, 64, 128, 256, 512]),\n        'max_bin': trial.suggest_int('max_bin', low=8, high=255),\n        'min_child_samples': trial.suggest_int('min_child_samples', low=0, high=20),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', low=0.01, high=1),\n        'bagging_freq': trial.suggest_int('bagging_freq', low=0, high=32),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', low=0.01, high=1),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', low=1e-8, high=12.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', low=1e-8, high=12.0),\n#         'num_iterations': trial.suggest_int('num_iterations', low=100, high=10000, step=100),\n        }\n\n        cv_split = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n        folds = cv_split.split(X=X_train[best_features], y=y_train) \n\n        dtrain = lgb.Dataset(\n            data=X_train[best_features], label=y_train,\n        )\n\n        lgbcv = lgb.cv(\n            params=params,\n            train_set=dtrain,\n            folds=folds,\n            seed=SEED,\n            metrics={METRIC},\n            verbose_eval=False,                   \n            early_stopping_rounds=25,                   \n            num_boost_round=5000,\n        )\n        metric_result = max(lgbcv[METRIC+'-mean'])\n\n#     fit = lgb.Dataset(\n#         X_train[best_features], y_train,\n#     )\n\n#     val = lgb.Dataset(\n#         X_val[best_features], y_val,\n#         reference=fit,\n#     )\n\n#     model = lgb.train(\n#         params=params,\n#         train_set=fit,\n#         num_boost_round=5000,\n#         valid_sets=(fit, val),\n#         valid_names=('train', 'val'),\n#         early_stopping_rounds=25,\n#         verbose_eval=False,\n#     )\n    \n#     metric_result = roc_auc_score(y_val, model.predict(X_val[best_features]))\n        return metric_result\n    \n    study.optimize(optuna_objective, n_trials=400, n_jobs=4, show_progress_bar=True)\n    \n    best_params = study.best_params.copy()\n    best_params['max_depth'] = -1\n    best_params['metric'] = METRIC\n    best_params['zero_as_missing'] = False\n    best_params['verbosity'] = -1\n    best_params['boosting_type'] = 'gbdt'\n    best_params['seed'] = SEED\n    best_params['is_unbalance'] = True\n    best_params['objective'] = 'binary'\n    \n    return study.best_params, study.best_value","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:06:41.869008Z","iopub.execute_input":"2022-01-23T20:06:41.869333Z","iopub.status.idle":"2022-01-23T20:06:41.884705Z","shell.execute_reply.started":"2022-01-23T20:06:41.869299Z","shell.execute_reply":"2022-01-23T20:06:41.884029Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"n_splits = 3","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:06:42.522957Z","iopub.execute_input":"2022-01-23T20:06:42.523392Z","iopub.status.idle":"2022-01-23T20:06:42.527506Z","shell.execute_reply.started":"2022-01-23T20:06:42.523359Z","shell.execute_reply":"2022-01-23T20:06:42.526812Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"TPE_sampler = TPESampler(seed=SEED, n_startup_trials=100)\nTPE_study = optuna.create_study(direction='maximize', sampler=TPE_sampler, study_name='TPE_study')\n\nbest_params = get_best_params(X_train, y_train, best_features, n_splits, TPE_study)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-23T20:06:42.890148Z","iopub.execute_input":"2022-01-23T20:06:42.890884Z","iopub.status.idle":"2022-01-23T20:51:15.381425Z","shell.execute_reply.started":"2022-01-23T20:06:42.890845Z","shell.execute_reply":"2022-01-23T20:51:15.379798Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"TPE_study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:15.385540Z","iopub.execute_input":"2022-01-23T20:51:15.386376Z","iopub.status.idle":"2022-01-23T20:51:15.395467Z","shell.execute_reply.started":"2022-01-23T20:51:15.386330Z","shell.execute_reply":"2022-01-23T20:51:15.394593Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"best_params = TPE_study.best_params.copy()\nbest_params['max_depth'] = -1\nbest_params['metric'] = METRIC\nbest_params['zero_as_missing'] = False\nbest_params['verbosity'] = -1\nbest_params['boosting_type'] = 'gbdt'\nbest_params['seed'] = SEED\nbest_params['is_unbalance'] = True\nbest_params['objective'] = 'binary'","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:35.478598Z","iopub.execute_input":"2022-01-23T20:51:35.479238Z","iopub.status.idle":"2022-01-23T20:51:35.486474Z","shell.execute_reply.started":"2022-01-23T20:51:35.479201Z","shell.execute_reply":"2022-01-23T20:51:35.485697Z"},"trusted":true},"execution_count":184,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(TPE_study)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:37.359165Z","iopub.execute_input":"2022-01-23T20:51:37.360030Z","iopub.status.idle":"2022-01-23T20:51:37.645262Z","shell.execute_reply.started":"2022-01-23T20:51:37.359993Z","shell.execute_reply":"2022-01-23T20:51:37.644300Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(TPE_study)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:37.647286Z","iopub.execute_input":"2022-01-23T20:51:37.647560Z","iopub.status.idle":"2022-01-23T20:51:38.282262Z","shell.execute_reply.started":"2022-01-23T20:51:37.647525Z","shell.execute_reply":"2022-01-23T20:51:38.281275Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"# optuna.visualization.plot_param_importances(TPE_study)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:42.355457Z","iopub.execute_input":"2022-01-23T20:51:42.355788Z","iopub.status.idle":"2022-01-23T20:51:42.360060Z","shell.execute_reply.started":"2022-01-23T20:51:42.355734Z","shell.execute_reply":"2022-01-23T20:51:42.359231Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"markdown","source":"## Model training - CV","metadata":{}},{"cell_type":"markdown","source":"### 1. Find best **num_rounds**","metadata":{}},{"cell_type":"code","source":"def get_num_boost_rounds(X_train, y_train, best_features, best_params, n_splits):\n    cv_split = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    folds = cv_split.split(X=X_train[best_features], y=y_train) \n\n    dtrain = lgb.Dataset(\n        data=X_train[best_features], label=y_train,\n    )\n\n    lgbcv = lgb.cv(\n        params=best_params,\n        train_set=dtrain,\n        folds=folds,\n        seed=SEED,\n        metrics={METRIC},\n        verbose_eval=False,                   \n        early_stopping_rounds=25,                   \n        num_boost_round=8000,\n    )\n\n    num_boost_round = np.argmax(lgbcv[METRIC+'-mean']) + 1 # !!!!!!\n    print('num_boost_round:', num_boost_round)\n\n    return num_boost_round, lgbcv","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:43.279079Z","iopub.execute_input":"2022-01-23T20:51:43.279857Z","iopub.status.idle":"2022-01-23T20:51:43.288063Z","shell.execute_reply.started":"2022-01-23T20:51:43.279817Z","shell.execute_reply":"2022-01-23T20:51:43.286944Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"code","source":"num_boost_round, num_boost_result = get_num_boost_rounds(X_train, y_train, best_features, best_params, n_splits)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:44.622008Z","iopub.execute_input":"2022-01-23T20:51:44.622343Z","iopub.status.idle":"2022-01-23T20:51:48.430711Z","shell.execute_reply.started":"2022-01-23T20:51:44.622300Z","shell.execute_reply":"2022-01-23T20:51:48.429774Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"markdown","source":"### 2. CV loop","metadata":{}},{"cell_type":"code","source":"def get_cv_result(X_train, y_train, n_splits, best_features, best_params, num_boost_round):\n    \n    cv_split = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n    folds = cv_split.split(X=X_train, y=y_train) \n\n    y_test_pred_history = list()\n    y_test_target_history = list()\n\n    y_train_pred_history = list()\n    y_train_target_history = list()\n\n    roc_auc_train_avg = list()\n    roc_auc_test_avg = list() \n\n    for train_index, test_index in tqdm(folds, position=0, leave=True):\n        X_train_fold = X_train.reset_index(drop=True).loc[train_index, best_features]\n        y_train_fold = y_train[train_index]\n\n        X_test_fold = X_train.reset_index(drop=True).loc[test_index, best_features]\n        y_test_fold = y_train[test_index]\n\n\n        print('Train/Val shapes:')\n        print((X_train_fold.shape, y_train_fold.shape), (X_test_fold.shape, y_test_fold.shape))\n\n        print('Train/Val bad rates:')\n        print(y_train_fold.mean(), y_test_fold.mean())\n\n        print('Train/Val bad`s count:')\n        print(y_train_fold.sum(), y_test_fold.sum())\n        print()\n\n        train_fold = lgb.Dataset(\n            X_train_fold,\n            label=y_train_fold,\n        )\n        test_fold = lgb.Dataset(\n            X_test_fold,\n            label=y_test_fold,\n            reference=train_fold,\n        )\n\n        model = lgb.train(\n            params=best_params,\n            train_set=train_fold,\n            num_boost_round=num_boost_round,\n            valid_sets=(train_fold, test_fold),\n            valid_names=('train', 'val'),\n            verbose_eval=False,\n        )\n\n        y_train_pred_fold = model.predict(X_train_fold)\n        y_test_pred_fold = model.predict(X_test_fold)\n\n        roc_auc_train_avg.append(roc_auc_score(y_train_fold, y_train_pred_fold))\n        roc_auc_test_avg.append(roc_auc_score(y_test_fold, y_test_pred_fold))\n\n        print('Train/Val ROC-AUC:')\n        print(roc_auc_score(y_train_fold, y_train_pred_fold), roc_auc_score(y_test_fold, y_test_pred_fold))\n\n        lgb.plot_importance(model, max_num_features=25);\n        plt.show()\n        print('*'*50)\n        print('*'*50)\n        print()\n\n    print('Average ROC-AUC Train:', np.mean(roc_auc_train_avg))\n    print('Average ROC-AUC Val:', np.mean(roc_auc_test_avg))\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:56.489026Z","iopub.execute_input":"2022-01-23T20:51:56.489359Z","iopub.status.idle":"2022-01-23T20:51:56.505873Z","shell.execute_reply.started":"2022-01-23T20:51:56.489324Z","shell.execute_reply":"2022-01-23T20:51:56.504810Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"get_cv_result(X_train, y_train, n_splits, best_features, best_params, num_boost_round)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:51:58.074781Z","iopub.execute_input":"2022-01-23T20:51:58.075452Z","iopub.status.idle":"2022-01-23T20:52:04.815541Z","shell.execute_reply.started":"2022-01-23T20:51:58.075416Z","shell.execute_reply":"2022-01-23T20:52:04.814831Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"markdown","source":"## Model training - on all train data","metadata":{}},{"cell_type":"code","source":"fit = lgb.Dataset(\n    X_train[best_features], y_train,\n)\n\nval = lgb.Dataset(\n    X_val[best_features], y_val,\n    reference=fit,\n)\n\nmodel = lgb.train(\n    params=best_params,\n    train_set=fit,\n    num_boost_round=num_boost_round,\n    verbose_eval=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:11.298734Z","iopub.execute_input":"2022-01-23T20:52:11.299090Z","iopub.status.idle":"2022-01-23T20:52:12.371646Z","shell.execute_reply.started":"2022-01-23T20:52:11.299054Z","shell.execute_reply":"2022-01-23T20:52:12.370635Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"print(f'ROC-AUC (TRAIN): {roc_auc_score(y_train, model.predict(X_train[best_features]))}')\nprint(f'ROC-AUC (VAL): {roc_auc_score(y_val, model.predict(X_val[best_features]))}')\ny_val_pred = model.predict(X_val[best_features])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:12.373762Z","iopub.execute_input":"2022-01-23T20:52:12.374110Z","iopub.status.idle":"2022-01-23T20:52:12.516067Z","shell.execute_reply.started":"2022-01-23T20:52:12.374073Z","shell.execute_reply":"2022-01-23T20:52:12.515077Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfpr, tpr, thresholds = roc_curve(y_val, y_val_pred)\nprint(roc_auc_score(y_val, y_val_pred))\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Threshold value is:\", optimal_threshold)\nplot_roc_curve(fpr, tpr)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:16.407163Z","iopub.execute_input":"2022-01-23T20:52:16.407462Z","iopub.status.idle":"2022-01-23T20:52:16.665910Z","shell.execute_reply.started":"2022-01-23T20:52:16.407429Z","shell.execute_reply":"2022-01-23T20:52:16.664956Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"print('*'*10 + 'TRAIN' + '*'*10)\nprint(classification_report(y_train, (model.predict(X_train[best_features]) > optimal_threshold).astype(int) ))\nprint()\n\nprint('*'*10 + 'VAL' + '*'*10)\nprint(classification_report(y_val, (model.predict(X_val[best_features]) > optimal_threshold).astype(int) ))\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:19.762639Z","iopub.execute_input":"2022-01-23T20:52:19.763195Z","iopub.status.idle":"2022-01-23T20:52:19.902928Z","shell.execute_reply.started":"2022-01-23T20:52:19.763155Z","shell.execute_reply":"2022-01-23T20:52:19.901896Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(y_train, (model.predict(X_train[best_features]) > optimal_threshold).astype(int),),\n)\ndisp.plot(xticks_rotation='vertical')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:22.680180Z","iopub.execute_input":"2022-01-23T20:52:22.680504Z","iopub.status.idle":"2022-01-23T20:52:23.041266Z","shell.execute_reply.started":"2022-01-23T20:52:22.680470Z","shell.execute_reply":"2022-01-23T20:52:23.040302Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(\n    confusion_matrix=confusion_matrix(y_val, (model.predict(X_val[best_features]) > optimal_threshold).astype(int),),\n)\ndisp.plot(xticks_rotation='vertical')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:23.043016Z","iopub.execute_input":"2022-01-23T20:52:23.043238Z","iopub.status.idle":"2022-01-23T20:52:23.337171Z","shell.execute_reply.started":"2022-01-23T20:52:23.043208Z","shell.execute_reply":"2022-01-23T20:52:23.336233Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"markdown","source":"## Model training - final model","metadata":{}},{"cell_type":"code","source":"# df_train","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:34.896294Z","iopub.execute_input":"2022-01-23T20:52:34.896596Z","iopub.status.idle":"2022-01-23T20:52:34.900826Z","shell.execute_reply.started":"2022-01-23T20:52:34.896562Z","shell.execute_reply":"2022-01-23T20:52:34.899709Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"numerical_pipeline.fit(df_train);\nX_train_full = numerical_pipeline.transform(df_train).reset_index(drop=True)\ny_train_full = X_train_full[TARGET].values\nX_train_full.drop([WEEK, TARGET],axis=1, inplace=True)\nX_train_full = X_train_full.set_index(ID)\nX_train_full = X_train_full[best_features]\nX_train_full.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:39.974557Z","iopub.execute_input":"2022-01-23T20:52:39.975369Z","iopub.status.idle":"2022-01-23T20:52:50.702725Z","shell.execute_reply.started":"2022-01-23T20:52:39.975329Z","shell.execute_reply":"2022-01-23T20:52:50.701791Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"X_test = numerical_pipeline.transform(df_test).reset_index(drop=True)\nX_test.drop([WEEK],axis=1, inplace=True)\nX_test = X_test.set_index(ID)\nX_test = X_test[best_features]\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:52:54.739551Z","iopub.execute_input":"2022-01-23T20:52:54.739883Z","iopub.status.idle":"2022-01-23T20:53:03.329180Z","shell.execute_reply.started":"2022-01-23T20:52:54.739848Z","shell.execute_reply":"2022-01-23T20:53:03.328070Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"assert df_test[[ID]].drop_duplicates().shape[0] == X_test.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:53:58.274316Z","iopub.execute_input":"2022-01-23T20:53:58.274647Z","iopub.status.idle":"2022-01-23T20:53:58.282963Z","shell.execute_reply.started":"2022-01-23T20:53:58.274612Z","shell.execute_reply":"2022-01-23T20:53:58.282040Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"fit = lgb.Dataset(\n    X_train_full, y_train_full,\n)\n\nmodel_full = lgb.train(\n    params=best_params,\n    train_set=fit,\n    num_boost_round=num_boost_round,\n    verbose_eval=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:01.262678Z","iopub.execute_input":"2022-01-23T20:54:01.263006Z","iopub.status.idle":"2022-01-23T20:54:02.435344Z","shell.execute_reply.started":"2022-01-23T20:54:01.262973Z","shell.execute_reply":"2022-01-23T20:54:02.434255Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"pickle.dump(model_full, open(PATH2OUTPUT / 'lgb_class_weight_model_upd.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:05.368032Z","iopub.execute_input":"2022-01-23T20:54:05.369084Z","iopub.status.idle":"2022-01-23T20:54:05.386657Z","shell.execute_reply.started":"2022-01-23T20:54:05.369034Z","shell.execute_reply":"2022-01-23T20:54:05.385814Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"markdown","source":"## Model interpreting & Feature importance","metadata":{}},{"cell_type":"code","source":"lgb.plot_importance(model_full, max_num_features=30);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:07.998004Z","iopub.execute_input":"2022-01-23T20:54:07.998293Z","iopub.status.idle":"2022-01-23T20:54:08.710761Z","shell.execute_reply.started":"2022-01-23T20:54:07.998260Z","shell.execute_reply":"2022-01-23T20:54:08.709707Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model_full)\nshap_values_0 = explainer.shap_values(X_train_full)[0]\n# shap_values_1 = explainer.shap_values(X_train_full)[1]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:12.691499Z","iopub.execute_input":"2022-01-23T20:54:12.691818Z","iopub.status.idle":"2022-01-23T20:54:14.346056Z","shell.execute_reply.started":"2022-01-23T20:54:12.691783Z","shell.execute_reply":"2022-01-23T20:54:14.344930Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values_0, X_train_full,) # axis_color='white'","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:19.092269Z","iopub.execute_input":"2022-01-23T20:54:19.092942Z","iopub.status.idle":"2022-01-23T20:54:21.191675Z","shell.execute_reply.started":"2022-01-23T20:54:19.092893Z","shell.execute_reply":"2022-01-23T20:54:21.190727Z"},"trusted":true},"execution_count":209,"outputs":[]},{"cell_type":"code","source":"# shap.summary_plot(shap_values_1, X_train_full,) # axis_color='white'","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:50.203799Z","iopub.execute_input":"2022-01-23T20:54:50.204123Z","iopub.status.idle":"2022-01-23T20:54:50.209038Z","shell.execute_reply.started":"2022-01-23T20:54:50.204088Z","shell.execute_reply":"2022-01-23T20:54:50.208010Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"X_test[best_features]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:54:52.574166Z","iopub.execute_input":"2022-01-23T20:54:52.574494Z","iopub.status.idle":"2022-01-23T20:54:52.616172Z","shell.execute_reply.started":"2022-01-23T20:54:52.574458Z","shell.execute_reply":"2022-01-23T20:54:52.615125Z"},"trusted":true},"execution_count":211,"outputs":[]},{"cell_type":"code","source":"y_test_pred = model_full.predict(X_test[best_features])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:00.744789Z","iopub.execute_input":"2022-01-23T20:55:00.745127Z","iopub.status.idle":"2022-01-23T20:55:00.799546Z","shell.execute_reply.started":"2022-01-23T20:55:00.745090Z","shell.execute_reply":"2022-01-23T20:55:00.798460Z"},"trusted":true},"execution_count":212,"outputs":[]},{"cell_type":"code","source":"# plt.hist(y_train_pred);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:06:33.256631Z","iopub.status.idle":"2022-01-23T20:06:33.257036Z","shell.execute_reply.started":"2022-01-23T20:06:33.256817Z","shell.execute_reply":"2022-01-23T20:06:33.256848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_val_pred);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:03.556847Z","iopub.execute_input":"2022-01-23T20:55:03.557414Z","iopub.status.idle":"2022-01-23T20:55:03.761482Z","shell.execute_reply.started":"2022-01-23T20:55:03.557354Z","shell.execute_reply":"2022-01-23T20:55:03.760487Z"},"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_test_pred);","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:03.921710Z","iopub.execute_input":"2022-01-23T20:55:03.922358Z","iopub.status.idle":"2022-01-23T20:55:04.168404Z","shell.execute_reply.started":"2022-01-23T20:55:03.922318Z","shell.execute_reply":"2022-01-23T20:55:04.167562Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"markdown","source":"## Save the result","metadata":{}},{"cell_type":"code","source":"X_test['Predicted'] = y_test_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:09.070639Z","iopub.execute_input":"2022-01-23T20:55:09.071111Z","iopub.status.idle":"2022-01-23T20:55:09.077041Z","shell.execute_reply.started":"2022-01-23T20:55:09.071069Z","shell.execute_reply":"2022-01-23T20:55:09.076087Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"submissions = X_test.reset_index()[[ID, 'Predicted']]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:11.367862Z","iopub.execute_input":"2022-01-23T20:55:11.368217Z","iopub.status.idle":"2022-01-23T20:55:11.377155Z","shell.execute_reply.started":"2022-01-23T20:55:11.368183Z","shell.execute_reply":"2022-01-23T20:55:11.376225Z"},"trusted":true},"execution_count":216,"outputs":[]},{"cell_type":"code","source":"submissions.to_csv(PATH2OUTPUT / 'submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:55:12.555260Z","iopub.execute_input":"2022-01-23T20:55:12.555563Z","iopub.status.idle":"2022-01-23T20:55:12.578717Z","shell.execute_reply.started":"2022-01-23T20:55:12.555530Z","shell.execute_reply":"2022-01-23T20:55:12.577890Z"},"trusted":true},"execution_count":217,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}